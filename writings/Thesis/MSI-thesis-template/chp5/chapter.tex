
\chapter{Analysis of Numerical Methods}
\label{chp:AnalNumMethod}
There are a variety of ways to analyse the numerical methods presented in this paper []. There are also a variety of properties that numerical methods can possess. Chief among these is convergence, which for linear partial differential equations can be broken up into consistency and stability using the Lax equivalence theorem []. 


For dispersive equations such as the Serre equations other more specific properties may be of interest such as the error in the dispersion relation introduced by the numerical methods as investigated in []. 

The main obstacle for analysing the numerical methods for the Serre equations is that the Serre equations are non-linear, which means the techniques employed to investigate the convergence and dispersion error of our numerical methods are no longer valid. However, some insights can still be gained by instead examining the linearised version of the Serre equations as has been done []. This is the approach we take in this section of the thesis.

In this thesis we will demonstrate the stability of the finite difference methods and calculate the dispersion error of the hybrid finite volume methods. In the finite difference methods we have a horizontal bed profile, while for the dispersion relation the bed terms have no contribution, so for the rest of this chapter we will assume $b(x) = 0$ for all $x$.

To linearise the equations we assume that 

\begin{align}
\label{eq:pertubation}
h(x,t) &= H + \delta \eta(x,t) + \mathcal{O}\left(\delta^2 \right), \\
u(x,t) &= U + \delta \upsilon(x,t) + \mathcal{O}\left(\delta^2 \right).
\end{align}
Where $\delta \ll 1$, so that we are modelling small waves on top of water with a mean height of $H$ and a background mean flow of $U$ and terms of order $\delta^2$ are negligible. We substitute this into \eqref{eqn:FullSerreCon} and \eqref{defn:SerreEqnConservedQuantity1} and neglect terms of order $\delta^2$ to obtain

\begin{subequations}
	\begin{gather}
		\label{eqn:LinCont}
		\frac{\partial  \eta}{\partial  t} + H\frac{\partial  \upsilon}{\partial  x} + U\frac{\partial  \eta}{\partial  x} +  = 0
	\end{gather}
	and
	\begin{gather}
	\label{eqn:LineMome}
	H\frac{\partial  \upsilon}{\partial  t} + gH\frac{\partial  \eta}{\partial  x} + UH\frac{\partial  \upsilon}{\partial  x} - \frac{H^3}{3}\left(U\frac{\partial^3  \upsilon}{\partial  x^3} + \frac{\partial^3  \upsilon}{\partial  x^3 \partial  t}  \right)  = 0
	\end{gather}
\label{eqn:LinSerre}	
with
\begin{gather}
	G = U\left(H + \eta\right) + H\upsilon -\frac{H^3}{3} \frac{\partial^2 \upsilon}{\partial x^2}.
	\label{eqn:LinConSerreG}
\end{gather}	
\end{subequations}
These are the linearised Serre equations.

%Phillipine stuff in here%
\section{Dispersion Error}
To study the error in the dispersion relation caused by the numerical methods we will follow the work of \cite{Filippini-etal-2016-381} who used a range of numerical methods on a different reformulation of the Serre equations $U = 0$, this is a reasonable simplification because firstly we are interested in modelling waves on quiescent water and secondly the more complicated part of the dispersion error to correctly approximate is the term which dictates the behaviour of gravity waves on still water.

By assuming that $U= 0$ the equations \eqref{eqn:LinSerre} and \eqref{eqn:LinConSerreG} reduce to

\begin{subequations}
	\label{eqn:LinSerreu0}
	\begin{gather}
	\label{eqn:LinContu0}
	\frac{\partial  \eta}{\partial  t} + H\frac{\partial  \upsilon}{\partial  x} = 0
	\end{gather}
	and
	\begin{gather}
	\label{eqn:LineMomeu0}
	h_0\frac{\partial  \upsilon}{\partial  t} + g H \frac{\partial  \eta}{\partial  x} - \frac{H^3}{3}\left(\frac{\partial^3  \upsilon}{\partial  x^3 \partial  t}  \right)  = 0
	\end{gather}	
with
	\begin{gather}
	G = H\upsilon -\frac{H^3}{3} \frac{\partial^2 \upsilon}{\partial x^2}.
	\label{eqn:LinConSerreGu0}
	\end{gather}	
\end{subequations}

The linearised equations \eqref{eqn:LinSerreu0} can be reformulated into equations with $\eta$ and $G$ as conserved variables as in \eqref{eqn:FullSerreCon} to obtain
\begin{subequations}
	\begin{gather}
	\label{eqn:LinContG}
	\frac{\partial  \eta}{\partial  t} + H\frac{\partial  \upsilon}{\partial  x} = 0,
	\end{gather}
	
	\begin{gather}
	\label{eqn:LineMomeG}
	\frac{\partial  G}{\partial  t} + g H \frac{\partial  \eta}{\partial  x}  = 0.
	\end{gather}
	\label{eqn:LinSerreG}	
\end{subequations}

For brevity we will only demonstrate our analysis of the dispersion error for our hybrid finite volume methods, through singular examples for the steps required to finally attain the dispersion error. In particular we will demonstrate how the dispersion error was obtained for both the second-order finite difference volume method and the second-order finite element volume method. 

To perform the dispersion error we assume that $\eta$ and $\upsilon$ are periodic functions in both space and time. In particular we assume that these both these quantities are fourier modes, which for some quantity $q$ is given by
\begin{equation}
q(x,t) = q(0,0) e^{i\left(\omega t + kx\right)}
\label{eqn:FourierNode}
\end{equation}


Therefore because we use uniform spatial grids so that $q^n_j = q(x_j,t^n)$  we have that
\begin{align}
q^{n}_{j \pm l} = q^n_j e^{\pm ik l\Delta x} & & \text{and} & & q^{n \pm l}_{j} = q^n_j e^{\pm i \omega l\Delta t}
\label{eqn:fourierfactor}
\end{align}

%give an outline here of the general principles, each numerical process introduces some error factor

%continous in time, discretisation in space

For the hybrid finite volume methods we break this process up into the three parts of these methods, the elliptic equation which relates the $h$ and $G$ to $u$, the evolution equation we use to update $h$ and $G$ and the Runge-Kutta steps we use to increase the order of accuracy of the method in time. 

\subsection{Elliptic Equation}

\subsubsection{Finite Difference}
The elliptic equation \eqref{eqn:LinConSerreGu0} at a particular grid point $x_j$ is

\[G_j = H\upsilon_j -\frac{H^3}{3} \left(\frac{\partial^2 \upsilon}{\partial x^2}\right)_j\]

For the second-order finite difference method the derivative $\frac{\partial^2 \upsilon}{\partial x^2}$ is approximated by   

\[ \left(\frac{\partial^2 \upsilon}{\partial x^2}\right)_j = \frac{\upsilon_{j-1} - 2\upsilon_{j} + \upsilon_{j+1}}{\Delta x^2}.\]

Making use of \eqref{eqn:fourierfactor} this becomes

\[ \left(\frac{\partial^2 \upsilon}{\partial x^2}\right)_j = \frac{\upsilon_{j} e^{-ik\Delta x} - 2\upsilon_{j} + \upsilon_{j}e^{ik\Delta x}}{\Delta x^2}.\]

Which reduces to

\[ \left(\frac{\partial^2 \upsilon}{\partial x^2}\right)_j = \frac{ 2\cos\left(k\Delta x\right) - 2 }{\Delta x^2} \upsilon_{j}.\]

Substituting this approximation into our elliptic equation \eqref{eqn:LinConSerreGu0} one obtains

\[G_j = \left(H -\frac{H^3}{3} \frac{ 2\cos\left(k\Delta x\right) - 2 }{\Delta x^2}\right) \upsilon_{j}.\] 

Therefore we an equation which is independent of $j$ which gives the error introduced transforming between $G_j$ and $u_j$ of this form
\[G_j = \mathcal{G} \upsilon_{j}.\]

In particular for the centred second-order finite difference method we have
\[\mathcal{G}_{FD2} = \left(H -\frac{H^3}{3} \frac{ 2\cos\left(k\Delta x\right) - 2 }{\Delta x^2}\right).\]
Where the subscript denotes that it is the factor for the second-order finite different approximation to this transformation.

\subsubsection{Finite Element Method} 
Since finite difference methods are all very similar in how the error coefficient is found, it is sufficient to just show one example of the process used. Finite element methods however, are slightly different and so we now include the working done to obtain the error coefficient of the FEM as well for our mixed P1 P2 FEM. 

Instead of a relation between $G_j$ and $\upsilon_j$ as in the finite difference method above, we get $\upsilon_{j+1/2}$ by solving the finite element method. So we desire the error coefficient produced by the FEM solution of the elliptic equation given $G_j$ and solving for $\upsilon_{j+1/2}$ which will be used in the conservation equation part below as our reconstruction error coefficient for $\upsilon$ at the cell interface. The matrix equation of the FEM for the linearised equations \eqref{eqn:LinConSerreGu0} can be attained by just using $h^+_{j-/2} = H =h^-_{j+1/2} $ and so we obtain from []

\begin{multline*}
\sum_j \frac{\Delta x}{6}\begin{bmatrix} G^+_{j -1/2} \\2 G^+_{j -1/2}+2 G^-_{j +1/2} \\ G^-_{j +1/2} \end{bmatrix} = \\\sum_j \left(H\frac{\Delta x}{30}\begin{bmatrix} 4 &2 &-1 \\2 &16 &2  \\-1 &2 &4 \end{bmatrix} + \frac{H^3 }{9\Delta x}\begin{bmatrix} 7 &-8 &1  \\-8 &16 &-8  \\1 &-8 &7  \end{bmatrix} \right) \begin{bmatrix} \upsilon_{j -1/2} \\\upsilon_{j} \\ \upsilon_{j +1/2} \end{bmatrix}
\end{multline*}

%minmod limiter for G
 Using our relations from the periodic nature of $u$ and $G$, and the minmod reconstruction used on $G$ we get that
 
 \begin{multline}
 \sum_j \frac{\Delta x}{6}\begin{bmatrix} e^{-ik\Delta x} \mathcal{R}^+_2 \\2 e^{-ik\Delta x} \mathcal{R}^+_2 +2 \mathcal{R}^-_2 \\ \mathcal{R}^-_2 \end{bmatrix} G_j = \\\sum_j \Bigg(H\frac{\Delta x}{30}\begin{bmatrix} 5i\sin\left(k \frac{\Delta x}{2}\right) + 3\cos\left(k \frac{\Delta x}{2}\right) + 2\\16 + 4 \cos\left(k \frac{\Delta x}{2}\right) \\ -5i\sin\left(k \frac{\Delta x}{2}\right) + 3\cos\left(k \frac{\Delta x}{2}\right) + 2 \end{bmatrix} \\+ \frac{H^3 }{9\Delta x}\begin{bmatrix} 6i\sin\left(k \frac{\Delta x}{2}\right) + 8\cos\left(k \frac{\Delta x}{2}\right) - 8 \\ - 16\cos\left(k \frac{\Delta x}{2}\right) + 16 \\ -6i\sin\left(k \frac{\Delta x}{2}\right) + 8\cos\left(k \frac{\Delta x}{2}\right) - 8 \end{bmatrix}  \Bigg) \upsilon_j
 \end{multline}
 
 We can now add all the terms that overlap i.e the extra contributions from the functions $\phi_{j+1/2}$ and $\phi_{j-1/2}$ from outside the cell $\left[x_{j-1/2}, x_{j+1/2}\right]$, this then gives us a relation between the sub-vectors of the total vectors of the FEM. Doing this we can rewrite the matrix equation as
 %don't like this notation
 []
 \begin{multline}
 \sum_j \frac{\Delta x}{6}\begin{bmatrix} 2  \\ \mathcal{R}^-_2 + \mathcal{R}^+_2 \end{bmatrix}^T \begin{bmatrix} G_j  \\ G_j\end{bmatrix}  = \\\sum_j \Bigg(H\frac{\Delta x}{30}\begin{bmatrix}16 + 4 \cos\left(k \frac{\Delta x}{2}\right) \\ 4\cos\left(\frac{k\Delta x}{2}\right) + 8 \cos\left(k \Delta x\right) - 2\end{bmatrix}^T  \\+ \frac{H^3 }{9\Delta x}\begin{bmatrix}  - 16\cos\left(k \frac{\Delta x}{2}\right) + 16 \\ -16\cos\left(\frac{k\Delta x}{2}\right) + 14 \cos\left(k \Delta x\right) + 2 \end{bmatrix}^T    \Bigg) \begin{bmatrix} u_j  \\ u_{j+1/2} \end{bmatrix}
 \end{multline}
 
 %diagonal matrix with the eigenvalues
 
 So the equation for $u_{j+1/2}$ is
 
 \begin{multline}
 \frac{\Delta x}{6}\left(\mathcal{R}^+_2 +\mathcal{R}^-_2\right)  G_j = \\ \Bigg(H\frac{\Delta x}{30} \left(4\cos\left(\frac{k\Delta x}{2}\right) + 8 \cos\left(k \Delta x\right) - 2 \right) \\+ \frac{H^3 }{9\Delta x}\left(-16\cos\left(\frac{k\Delta x}{2}\right) + 14 \cos\left(k \Delta x\right) + 2 \right)   \Bigg)  u_{j + 1/2}
 \end{multline}
 
 so we have
 
 \begin{multline}
 G_j = \\ \frac{6}{\Delta x\left(\mathcal{R}^+_2 +\mathcal{R}^-_2\right)} \Bigg(H\frac{\Delta x}{30} \left(4\cos\left(\frac{k\Delta x}{2}\right) + 8 \cos\left(k \Delta x\right) - 2 \right) \\+ \frac{H^3 }{9\Delta x}\left(-16\cos\left(\frac{k\Delta x}{2}\right) + 14 \cos\left(k \Delta x\right) + 2 \right)   \Bigg) e^{i k\frac{\Delta x}{2}}  u_{j}
 \end{multline}
 
 So we have 
  \begin{multline}
  \mathcal{G}_{FEMP1P2}  = \\ \frac{6}{\Delta x\left(\mathcal{R}^+_2 +\mathcal{R}^-_2\right)} \Bigg(H\frac{\Delta x}{30} \left(4\cos\left(\frac{k\Delta x}{2}\right) + 8 \cos\left(k \Delta x\right) - 2 \right) \\+ \frac{H^3 }{9\Delta x}\left(-16\cos\left(\frac{k\Delta x}{2}\right) + 14 \cos\left(k \Delta x\right) + 2 \right)   \Bigg) e^{i k\frac{\Delta x}{2}} 
  \end{multline}
 This is the error introduced by calculating $u_{j+1/2}$ in our method, the only $u$ value we need to numerically solve the linearised Serre equations. 


\subsection{Conservation Equation}

Finite volume methods have the following update scheme to approximate equations in conservation law form [] for some quantity $q$
\begin{equation}
\label{eqn:ConUpdateForm}
\bar{q}^{\,n + 1}_{j} = \bar{q}^{\,n}_{j} - \frac{\Delta t}{\Delta x} \left[F^{\,n} _{j+1/2} - F^{\,n} _{j-1/2} \right].
\end{equation}


Where the bar denotes that it is the cell average of the quantity $q$ and $F^{\,n} _{j+1/2}$ and $F^{\,n} _{j-1/2}$ are the approximations to the average fluxes across the cell boundary between the times $t^n$ and $t^{n+1}$. 

In our methods there is some transformation between the nodal value $q_j$ and the cell average $\bar{q}_j$, which will introduce some error factor $\mathcal{M}$. For first and second order methods $\mathcal{M}_1 = \mathcal{M}_2 = 1$, however for higher-order methods $\mathcal{M} \neq 1$.

To calculate the fluxes $F^{\,n} _{j+1/2}$ and $F^{\,n} _{j-1/2}$ we use Kurganov's method \cite{Kurganov-etal-2001-707} which is 
\begin{equation*}
F_{j+\frac{1}{2}} = \dfrac{a^+_{j+\frac{1}{2}} f\left(q^-_{j+\frac{1}{2}}\right) - a^-_{j+\frac{1}{2}} f\left(q^+_{j+\frac{1}{2}}\right)}{a^+_{j+\frac{1}{2}} - a^-_{j+\frac{1}{2}}}  + \dfrac{a^+_{j+\frac{1}{2}} \, a^-_{j+\frac{1}{2}}}{a^+_{j+\frac{1}{2}} - a^-_{j+\frac{1}{2}}} \left [ q^+_{j+\frac{1}{2}} - q^-_{j+\frac{1}{2}} \right ]
\end{equation*}

where $a^+_{j+\frac{1}{2}}$ and $a^-_{j+\frac{1}{2}}$ are given by the wave speed bounds [], so that 

\[a^-_{j+ 1/2} =  - \sqrt{g H}\]

\[a^+_{j+ 1/2} = \sqrt{g H}.\]
We have suppressed the superscripts denoting the time to simplify the notation as all times are now $t^n$ in the flux calculation. Substituting these values into the flux approximation we obtain 

\begin{equation}\label{eqn:HLL_fluxred}
F_{j+\frac{1}{2}} = \dfrac{ f\left(q^-_{j+\frac{1}{2}}\right) + f\left(q^+_{j+\frac{1}{2}}\right)}{ 2}  - \dfrac{ \sqrt{gH}}{ 2} \left [ q^+_{j+\frac{1}{2}} - q^-_{j+\frac{1}{2}} \right ]
\end{equation}

For $\eta$ our Kurganov approximation to the flux of \eqref{eqn:LinContG} is then

\begin{equation}
\label{eqn:HLL_fluxeta}
F^{\eta}_{j+\frac{1}{2}} = \dfrac{ H \upsilon ^-_{j+\frac{1}{2}}+ H \upsilon ^+_{j+\frac{1}{2}}}{ 2}  - \dfrac{ \sqrt{gH}}{ 2} \left [ \eta^+_{j+\frac{1}{2}} - \eta^-_{j+\frac{1}{2}} \right ]
\end{equation}

The missing pieces here are the errors introduced by reconstruction of the edge values $\upsilon ^-_{j+\frac{1}{2}}$, $\upsilon ^+_{j+\frac{1}{2}}$, $\eta ^-_{j+\frac{1}{2}}$ and $\eta ^+_{j+\frac{1}{2}}$ from the cell averages $\bar{\upsilon}_j$ and $\bar{\eta}_j$. Because our quantities are smooth the nonlinear limiters can be neglected so we have for the second-order reconstruction of $\eta$ 

\begin{equation*}
\eta^-_{j+\frac{1}{2}} = \bar{\eta}_j + \frac{- \bar{\eta}_{j - 1} + \bar{\eta}_{j+ 1} }{4}
\end{equation*}
\begin{equation*}
\eta^+_{j+\frac{1}{2}} = \bar{\eta}_{j+1} + \frac{- \bar{\eta}_{j} + \bar{\eta}_{j+ 2}}{4}.
\end{equation*}
	
Using \eqref{eqn:fourierfactor} these equations become


\begin{equation*}
\eta^-_{j+\frac{1}{2}} = \mathcal{M}_2{\eta}_j + \frac{- \mathcal{M}_2{\eta}_{j} e^{-ik\Delta x} + \mathcal{M}_2{\eta}_{j} e^{ik\Delta x}}{4}
\end{equation*}
\begin{equation*}
\eta^+_{j+\frac{1}{2}} = \mathcal{M}_2{\eta}_{j}e^{ik\Delta x} + \frac{- \mathcal{M}_2{\eta}_{j} + \mathcal{M}_2{\eta}_{j}e^{2ik\Delta x} }{4}.
\end{equation*}


For the second order case $\mathcal{M}_2 = 1$ and these equations can be reduced to 

\begin{subequations}
	\label{eqn:2ndrecon}
	\begin{equation}
	\eta^-_{j+\frac{1}{2}} = \left(1  + \frac{i\sin\left(k\Delta x\right)}{2} \right){\eta}_j
	\end{equation}
	\begin{equation}
	\eta^+_{j+\frac{1}{2}} = e^{ik\Delta x}\left(1  - \frac{i\sin\left(k\Delta x\right)}{2} \right){\eta}_{j}.
	\end{equation}
\end{subequations}

From these we introduce the second order reconstruction factors $\mathcal{R}^+_2 = e^{ik\Delta x}\left(1  - \frac{i\sin\left(k\Delta x\right)}{2} \right)$ and $\mathcal{R}^-_2 = 1  + \frac{i\sin\left(k\Delta x\right)}{2}$ for both $\eta$ and $G$. So that we have 

\begin{equation*}
\eta^-_{j+\frac{1}{2}} = \mathcal{R}^-_2 {\eta}_j
\end{equation*}
\begin{equation*}
\eta^+_{j+\frac{1}{2}} = \mathcal{R}^+_2{\eta}_{j}.
\end{equation*}

In our numerical methods our reconstruction of $\upsilon$ is slightly different as $\upsilon ^-_{j+\frac{1}{2}}$ and $\upsilon ^+_{j+\frac{1}{2}}$ are equal as we assume $\upsilon$ is continuous. For the second order finite difference volume method we have

\begin{equation*}
u^-_{j + 1/2} = u^+_{j + 1/2} = \frac{u_{j+1} + u_{j}}{2}
\end{equation*}

Using \eqref{eqn:fourierfactor} and rearranging gives

\begin{equation}
\label{eqn:2ndreconu}
u^-_{j + 1/2} = u^+_{j + 1/2} = \frac{e^{ik\Delta x } + 1}{2} u_{j}.
\end{equation}

We therefore introduce the second order reconstruction error factor $\mathcal{R}^u_{FD2} = \frac{e^{ik\Delta x } + 1}{2}$. Because our FEM for the elliptic equation [] calculates $u_{j+1/2}$ from $G_j$ we do not need to reconstruct $u_{j+1/2}$ and so we have the $\mathcal{R}^u_{FEM2} = e^{ik\frac{\Delta x}{2} }$, which corresponds to calculating $u_{j+1/2}$ analytically given $u_{j}$. 

We will now suppress the superscripts and subscripts in the flux approximation, as the rest of the calculations are independent of the particular reconstructions used. Substituting these error coefficients into into \eqref{eqn:HLL_fluxeta} results in
\begin{equation*}
F^{\eta}_{j+\frac{1}{2}} = \dfrac{ H  \mathcal{R}^u \upsilon_{j}+ H\mathcal{R}^u \upsilon_{j}}{ 2}  - \dfrac{ \sqrt{gH}}{ 2} \left [  \mathcal{R}^+ {\eta}_j -  \mathcal{R}^-{\eta}_j \right ]
\end{equation*}

Which becomes

\begin{equation*}
F^{\eta}_{j+\frac{1}{2}} = H \mathcal{R}^u \upsilon_{j}   - \dfrac{ \sqrt{gH}}{ 2} \left [  \mathcal{R}^+ -  \mathcal{R}^- \right ] {\eta}_j
\end{equation*}

We then introduce the factors $\mathcal{F}^{\eta,\upsilon}$ and $\mathcal{F}^{\eta,\eta}$
\begin{align*}
\mathcal{F}^{\eta,\eta} & = - \dfrac{ \sqrt{gH}}{ 2} \left [  \mathcal{R}^+ -  \mathcal{R}^- \right ]\\
\mathcal{F}^{\eta,\upsilon} & = H \mathcal{R}^u
\end{align*}
so that
\begin{equation}
\label{eqn:etafluxapprox}
F^{\eta}_{j+\frac{1}{2}} = \mathcal{F}^{\eta,\upsilon} \upsilon_{j}   +  \mathcal{F}^{\eta,\eta} {\eta}_j.
\end{equation}

Repeating this process for $G$ using [] and [] we get that
\begin{equation}
\label{eqn:Gfluxapprox}
F^{G}_{j+\frac{1}{2}} =  \mathcal{F}^{G,\eta} \eta_{j}  + \mathcal{F}^{G,\upsilon} \upsilon_j
\end{equation}

\begin{align*}
\mathcal{F}^{G,\eta} & = gH \dfrac{\mathcal{R}^-_2 + \mathcal{R}^+_2 }{ 2} \\
\mathcal{F}^{G,\upsilon} & = - \dfrac{ \sqrt{gH}}{ 2} \left [  \mathcal{R}^+ -  \mathcal{R}^- \right ] \mathcal{G}
\end{align*}


By substituting \eqref{eqn:etafluxapprox}, \eqref{eqn:Gfluxapprox} into \eqref{eqn:ConUpdateForm} our finite volume method can be written as

\begin{equation*}
\mathcal{M}_2 \eta^{\,n + 1}_{j} = \mathcal{M}_2 \eta^{\,n }_{j} - \frac{\Delta t}{\Delta x}  \left[ \left(1 - e^{ik\Delta x}\right) \left(\mathcal{F}_2^{\eta,\eta} h_{j}  + \mathcal{F}_2^{\eta,\upsilon} \upsilon_j \right) \right]
\end{equation*}
\begin{equation*}
\mathcal{M}_2 G^{\,n + 1}_{j} = \mathcal{M}_2 G^{\,n }_{j} - \frac{\Delta t}{\Delta x}  \left[ \left(1 - e^{ik\Delta x}\right) \left(  \mathcal{F}_2^{G,\eta} \eta_{j}  + \mathcal{F}_2^{G,\upsilon} \upsilon_j \right) \right]
\end{equation*}
	
Furthermore by transforming the $G$'s into $\upsilon$'s using our second order finite volume factor $\mathcal{G}_{FD2}$ (or the FEM one) [] we obtain
	
\begin{align*}
\eta^{\,n + 1}_{j} &= \eta^{\,n }_{j} - \frac{1}{\mathcal{M}_2}\frac{\Delta t}{\Delta x}  \left[ \left(1 - e^{ik\Delta x}\right) \left(\mathcal{F}_2^{\eta,\eta} \eta_{j}  + \mathcal{F}_2^{\eta,\upsilon} \upsilon_j \right) \right] \\
\upsilon^{\,n + 1}_{j} &= \upsilon^{\,n }_{j} -  \frac{1}{\mathcal{G}_{FD2} \mathcal{M}_2}\frac{\Delta t}{\Delta x}  \left[ \left(1 - e^{ik\Delta x}\right) \left(  \mathcal{F}_2^{G,\eta} \eta_{j}  + \mathcal{F}_2^{G,\upsilon} \upsilon_j \right) \right]
\end{align*}

This can be written in matrix form as

\begin{equation*}
\left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n+1}_j = \left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j - \frac{\left(1 - e^{ik\Delta x}\right) \Delta t}{\Delta x}\left[\begin{array}{c c}
\frac{1}{\mathcal{M}_2} \mathcal{F}_2^{\eta,\eta} & \frac{1}{\mathcal{M}_2}\mathcal{F}_2^{\eta,\upsilon} \\ \frac{1}{\mathcal{G}_{FD2} \mathcal{M}_2}\mathcal{F}_2^{\upsilon,\eta} & \frac{1}{\mathcal{G}_{FD2} \mathcal{M}_2}\mathcal{F}_2^{\upsilon,\upsilon} 
\end{array}\right]\left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j
\end{equation*}

Introducing 

\[\boldsymbol{F}_2 = \frac{\left(1 - e^{ik\Delta x}\right)}{\Delta x}\left[\begin{array}{c c}
\mathcal{F}_2^{\eta,\eta} & \mathcal{F}_2^{\eta,\upsilon} \\ \frac{1}{\mathcal{G}}\mathcal{F}_2^{\upsilon,\eta} &  \frac{1}{\mathcal{G}}\mathcal{F}_2^{\upsilon,\upsilon} 
\end{array}\right] \]

this becomes

\begin{equation*}
\label{eqn:matrixevolupdate}
\left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n+1}_j = \left(\boldsymbol{I}  - \Delta t \boldsymbol{F}_2 \right) \left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j
\end{equation*}

\subsection{Runge-Kutta Time Stepping}
The above analysis does not include the Runge-Kutta steps that make allow our schemes to be higher order in time. However, extending this analysis to Runge-Kutta steps is not difficult now that our method is in the form \eqref{eqn:matrixevolupdate}. For second order time stepping the Runge Kutta steps are then

\begin{subequations}
	\label{eqn:RKstepfull}
	\begin{equation}
	\label{eqn:RKstepfullp1}
	\left[\begin{array}{c}
	\eta \\ \upsilon
	\end{array}\right]^{1}_j = \left(\boldsymbol{I} - \Delta t\boldsymbol{F}_2 \right)\left[\begin{array}{c}
	\eta \\ \upsilon
	\end{array}\right]^{n}_j
	\end{equation}
	
	\begin{equation}
	\label{eqn:RKstepfullp2}
	\left[\begin{array}{c}
	\eta \\ \upsilon
	\end{array}\right]^{2}_j = \left(\boldsymbol{I} - \Delta t\boldsymbol{F}_2 \right)\left[\begin{array}{c}
	\eta \\ \upsilon
	\end{array}\right]^{1}_j
	\end{equation}
		
	\begin{equation}
	\label{eqn:RKstepfullp3}
	\left[\begin{array}{c}
	\eta \\ \upsilon
	\end{array}\right]^{n+1}_j = \frac{1}{2} \left(\left[\begin{array}{c}
	\eta \\ \upsilon
	\end{array}\right]^{n}_j + \left[\begin{array}{c}
	\eta \\ \upsilon
	\end{array}\right]^{2}_j\right) 
	\end{equation}
\end{subequations}


Substituting \eqref{eqn:RKstepfullp1} and \eqref{eqn:RKstepfullp2} into \eqref{eqn:RKstepfullp3} gives
\begin{equation*}
\left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n+1}_j = \frac{1}{2} \left(\left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j + \left(\boldsymbol{I} - \Delta t\boldsymbol{F}_2 \right)^2 \left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j\right)
\end{equation*}

Expanding this we get

\begin{equation*}
\left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n+1}_j = \frac{1}{2} \left(2\boldsymbol{I}  -2\Delta t\boldsymbol{F}_2 + \Delta t^2\boldsymbol{F}^2_2 \right) \left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j 
\end{equation*}

lets say we have an eigenvalue decomposition $\boldsymbol{F}_2 = \boldsymbol{P} \boldsymbol{\Lambda} \boldsymbol{P}^{-1} $ then this can be rewritten as 

\begin{equation*}
\left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n+1}_j = \frac{1}{2} \left(2\boldsymbol{I}  -2\Delta t\boldsymbol{P} \boldsymbol{\Lambda} \boldsymbol{P}^{-1}  + \Delta t^2\boldsymbol{P} \boldsymbol{\Lambda}^2 \boldsymbol{P}^{-1} \right) \left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j 
\end{equation*}

Multiplying by $\boldsymbol{P}^{-1}$ on the left and rearranging this get that

\begin{equation*}
\boldsymbol{P}^{-1} \left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n+1}_j = \frac{1}{2} \left(2 -2\Delta t \boldsymbol{\Lambda}  + \Delta t^2 \boldsymbol{\Lambda}^2  \right)  \boldsymbol{P}^{-1} \left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j 
\end{equation*}

Since $\eta$ and $\upsilon$ are Fourier modes we have

\begin{equation*}
e^{i\omega \Delta t} \left(\boldsymbol{P}^{-1} \left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j  \right)= \left(1 -1\Delta t \boldsymbol{\Lambda}  + \frac{1}{2}\Delta t^2 \boldsymbol{\Lambda}^2  \right)  \left(\boldsymbol{P}^{-1} \left[\begin{array}{c}
\eta \\ \upsilon
\end{array}\right]^{n}_j  \right)
\end{equation*}

Since $\boldsymbol{\Lambda}$ is a diagonal matrix of the eigenvalues $\lambda_1$ and $\lambda_2$ we have that

\begin{align*}
e^{i\omega\Delta t} &= 1 + \frac{1}{2}\Delta t^2 \lambda_{1}^2  -\Delta t\lambda_{1}, \\
e^{i\omega\Delta t} &= 1 + \frac{1}{2}\Delta t^2 \lambda_{2}^2  -\Delta t\lambda_{2}
\end{align*}

So that the dispersion relation for the second order finite difference finite volume method is

\begin{align}
\label{eqn:DispersionRelationSecondOrder}
\omega &= \frac{1}{i \Delta t} \ln \left(1 + \frac{1}{2}\Delta t^2 \lambda_{1}^2  -\Delta t\lambda_{1}\right), \\
\omega &= \frac{1}{i \Delta t} \ln \left(1 + \frac{1}{2}\Delta t^2 \lambda_{2}^2  -\Delta t\lambda_{2}\right),
\end{align}

Comparing this with the dispersion relation \eqref{eqn:DispersionRelation} of the Serre equations we can then determine the error in dispersion caused by the particular method. We perform this computationally by finding the eigenvalues of $\boldsymbol{F}$, substituting them into \eqref{eqn:DispersionRelationSecondOrder} and comparing that to the dispersion relation of the Serre equations for a particular $H$ and $k$ value. 


\subsection{Results}


\section{Neumann Stability}

We again begin from the linearised Serre equations \eqref{eqn:LinSerre} and as for the hybrid finite volume methods we will only show the working for one example as the process is the same in both cases. For the Neumann Stability our example will be the naive second-order finite difference method $\mathcal{D}$ \eqref{eq:Dnumdef}. Again we being by replacing both $\eta$ and $\upsilon$ by Fourier nodes \eqref{eqn:FourierNode}.

Because our approximations to derivatives is consistent for $\mathcal{D}$ we will provide all the factors for the second order centred finite difference approximations to derivatives of some quantity $q$ generated by making use of \eqref{eqn:fourierfactor}.

\begin{align}
 &\left(\frac{\partial q}{\partial x}\right)^n_j &=& \frac{- q^n_{j-1} + q^n_{j+1}}{2 \Delta x} &=& \frac{i \sin\left(k \Delta x\right)}{\Delta x} q^n_j \\
 &\left(\frac{\partial^2 q}{\partial x^2}\right)^n_j &=& \frac{q^n_{j-1} - 2q^n_j + q^n_{j+1}}{\Delta x^2} &=& \frac{2 \cos\left(k \Delta x\right) - 2}{\Delta x^2} q^n_j \\
&\left(\frac{\partial^3 q}{\partial x^3}\right)^n_j &=& \frac{- q^n_{j-2}  + 2q^n_{j-1}  - 2q^n_{j+1} + q^n_{j+2}}{2\Delta x^3} &=& -4i\sin\left(k \Delta x\right)\frac{\sin^2\left(\frac{k \Delta x}{2}\right) }{\Delta x^3} q^n_j
\label{eqn:FDfactorlist}
\end{align} 

\begin{subequations}
	\begin{gather}
	\frac{\partial  \eta}{\partial  t} + H\frac{\partial  \upsilon}{\partial  x} + U\frac{\partial  \eta}{\partial  x} = 0
	\end{gather}
	
	\begin{gather}
	H\frac{\partial  \upsilon}{\partial  t} + gH\frac{\partial  \eta}{\partial  x} + UH\frac{\partial  \upsilon}{\partial  x} - \frac{H^3}{3}\left(U\frac{\partial^3  \upsilon}{\partial  x^3} + \frac{\partial^3  \upsilon}{\partial  x^3 \partial  t}  \right)  = 0
	\end{gather}	
\end{subequations}

The factors we get for the temporal derivatives are very similar to this. The numerical method $\mathcal{D}$ is just attained from replacing all the derivatives in \eqref{eqn:LinSerre} with the approximations in \eqref{eqn:FDfactorlist}. For the linearised equations the update formulas of $\mathcal{D}$ become

\begin{subequations}
	\begin{equation}
	\eta^{n+1}_j = \eta^{n-1}_j - \Delta t \left(U \frac{- \eta^{n}_{j-1} + \eta^{n}_{j+1} }{\Delta x} + H \frac{- \upsilon^{n}_{j-1} + \upsilon^{n}_{j+1}}{\Delta x}\right).
	\end{equation}
	\begin{multline}
	\upsilon^{n+1}_j - \frac{H^2}{3}\frac{\upsilon^{n+1}_{j-1} -2\upsilon^{n+1}_{j} +\upsilon^{n+1}_{j+1} }{\Delta x^2} 
	\\ =  \upsilon^{n-1}_j - \frac{H^2}{3}\frac{\upsilon^{n-1}_{j-1} -2\upsilon^{n-1}_{j} +\upsilon^{n-1}_{j+1}}{\Delta x^2}   \\+  \Delta t\left(- g\frac{-\eta^n_{j-1} + \eta^n_{j+1} }{\Delta x}   - U\frac{-\upsilon^n_{j-1} + \upsilon^n_{j+1} }{\Delta x} + \frac{H^2}{3}\left(U \frac{-\upsilon^{n}_{j-2} +2\upsilon^{n}_{j-1} -2\upsilon^{n}_{j+1} +\upsilon^{n}_{j+2}}{\Delta x^3}  \right)\right)  \\
	\end{multline}
\end{subequations}


Since we have assumed that $\eta$ and $\upsilon$ are fourier nodes, we can just replace the finite difference approximations with the appropriate factors from \eqref{eqn:FDfactorlist}. After some rearranging we get that


\begin{subequations}
	\begin{equation}
	\eta^{n+1}_j = \eta^{n-1}_j - \Delta t \left(U  \frac{i \sin\left(k \Delta x\right)}{\Delta x}\eta^n_j + H\frac{i \sin\left(k \Delta x\right)}{\Delta x} \upsilon^n_j \right),
	\end{equation}
	\begin{multline}
	\upsilon^{n+1}_j  =  \upsilon^{n-1}_j  -  \frac{3 \Delta x^2\Delta t}{3 \Delta x^2 -2{H^2} \left( \cos\left(k \Delta x\right) - 1 \right)}\bigg( g \frac{i \sin\left(k \Delta x\right)}{\Delta x}     \bigg) \eta^n_j\\ + U\frac{i \Delta t \sin\left(k \Delta x\right)}{\Delta x} \upsilon^n_j  \\
	\end{multline}
\end{subequations}

By setting

\begin{align}
&A_{0,0} = -  \frac{2 i\Delta t }{\Delta x} U\sin\left(k \Delta x\right)\\
&A_{0,1} = -  \frac{2 i\Delta t}{\Delta x} H \sin\left(k \Delta x\right) \\
&A_{1,0} = -\frac{6 gi \Delta x\Delta t}{3 \Delta x^2 -2{H^2} \left( \cos\left(k \Delta x\right) - 1 \right)}{ \sin\left(k \Delta x\right)} \\
&A_{1,1} =\frac{2i \Delta t }{\Delta x} U \sin\left(k \Delta x\right)
\end{align}


\begin{equation}
\begin{bmatrix}
\eta^{n+1}_j \\
\upsilon^{n+1}_j\\
\eta^{n}_j \\
\upsilon^{n}_j
\end{bmatrix} = 
\begin{bmatrix}
A_{0,0}  & A_{0,1}  & 1 &0 \\
A_{1,0}  & A_{1,1}  & 0 &1 \\
1  & 0  &0 &0 \\
0  & 1  &0 &0 
\end{bmatrix} \begin{bmatrix}
\eta^{n}_j \\
\upsilon^{n}_j\\
\eta^{n-1}_j \\
\upsilon^{n-1}_j
\end{bmatrix}
\end{equation}

This matrix is the growth matrix and if its spectral radius is less than $1$ then $\mathcal{D}$ is stable.


For $\mathcal{W}$ after following through with same process we get that

\begin{align*}
&B_{0,0} = 1 - \frac{\Delta t}{\Delta x}A_{1,0}H\frac{i\sin\left(k\Delta x\right)}{2} \\ &- \frac{\Delta t}{\Delta x}U\left(\left(i\sin\left(k\Delta x\right)\right) - \frac{\Delta t}{\Delta x}U\left(\cos\left(ik\Delta x\right) - 1\right)\right) \\
&B_{0,1} = - \frac{\Delta t}{\Delta x} \left[H\frac{i\sin\left(k\Delta x\right)}{2}A_{1,1}   -U\left(\frac{\Delta t}{\Delta x}H\left(\cos\left(ik\Delta x\right) - 1\right)\right) \right] \\
&B_{0,4} = - \frac{\Delta t}{\Delta x}H\frac{i\sin\left(k\Delta x\right)}{2} \\
&B_{1,0} = A_{1,0} \\
&B_{1,1} =A_{1,1}
\end{align*}

\[
\left[\begin{array}{c}
h^{n+1}_j \\
u^{n+1}_j\\
h^n_j \\
u^n_j
\end{array} \right] = \left[\begin{array}{c c c c}
B_{0,0} & B_{0,1} & 0 & B_{0,4} \\
B_{1,0} & B_{1,1}&0 & 1 \\
1&0&0&0\\
0&1&0&0
\end{array} \right]  \left[\begin{array}{c}
h^{n}_j \\
u^{n}_j\\
h^{n-1}_j \\
u^{n-1}_j
\end{array} \right] 
\]

Again if this matrix has a spectral radius less than $1$ then $\mathcal{W}$ is stable.